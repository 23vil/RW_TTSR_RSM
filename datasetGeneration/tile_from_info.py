"""
tile_from_info.py
-------
PSSR PIPELINE - STEP 2:
Generate your training data - create semi-synthetic training pairs using the crappification approach.

This script is for generating consistent, GPU-friendly training data, with the
guidance from the .csv file generated by 'gen_sample_info.py'.

Parameters:
-------
- out: Path, relative path where the dataset to be generated will be saved.
- info: Path, relative path to the 'guidemap' .csv file generated by
        'gen_sample_info.py'.
- tile: int, generated tile size.
- n_train: int, number of training tiles.
- n_valid: int, number of validation tiles.
- crap_func: str, crappifier to be used, 'no_crap' by default. Check
             utils/crappifiers.py for more details about
             available crappifiers. (optional)
- n_frames: int, number of frames of the crappified LR. 1 if singleframe,
            5 if multiframe. 1 by default. (optional)
- lr_type: str, training input, (s)ingle, (t) multi
           or (z) multi. 's' by default. (optional)
- scale: int, downsample size. 4 by default. (optional)
- ftypes: str, file types, ftypes allowed e.g. czi, tif. If specified, only
          the specified type will be processed. (optional)
- upsample: boolean, true if rescaling crappified tiles to its original size
            is needed, otherwise false. False by default. (optional)
- only: str, generate training data only from the specified categories. All
        categories are included by default. (optional)
- skip: str, generate training data from all categories EXCEPT the specified
        categories, e.g. ['cat1', 'cat2']. None is skipped by default. (optional)
- clean: boolean, TRUE if wiping existing data in the output folder first,
         FALSE by default. (optional)

Returns:
-------
- a .csv file that stores the metadata information about the cropped tiles.
- training image pairs/tiles will be saved on disk.
 (if multiframe, low resolution images will be saved as .npy)

Examples:
-------
Example 1: singleframe data generation
Generate 500 singleframe training pairs and 50 validation pairs with tile size
512x512. All categories are needed except 'mitotracker' folder. 'no_crap' is the
crappifier, which equals to downsample only.
python tile_from_info.py --out datasets --info live_mitotracker.csv --n_train 500
--n_valid 50 --n_frames 1 --lr_type s --tile 512 --skip ['mitotracker', 'neuron']
--crap_func 'new_crap_AG_SP'

Example 2: multiframe data generation
Generate 500 5-timeframe multiframe training pairs and 50 validation pairs with
tile size 512x512. Only category 'mitotracker' is needed. 'new_crap_AG_SP' is
the crappifier.
python tile_from_info.py --out datasets --info live_mitotracker.csv --n_train 500
--n_valid 50 --n_frames 5 --lr_type t --tile 512 --only mitotracker
--crap_func 'new_crap_AG_SP'
"""

import yaml
from fastai.script import *
from fastai.vision import *
from utils import *
from utils.crappifiers import *
from pathlib import Path
from fastprogress import master_bar, progress_bar
from time import sleep
import torchvision
import shutil
import PIL
import czifile
import glob
from PIL import Image
from skimage.transform import rescale
from skimage import filters
from skimage.util import random_noise
from scipy.ndimage.interpolation import zoom as npzoom
PIL.Image.MAX_IMAGE_PIXELS = 99999999999999

def need_cache_flush(tile_stats, last_stats):
    if last_stats is None: return True
    if tile_stats['HRfn'] != last_stats['HRfn'] and tile_stats['LRfn'] != last_stats['LRfn']: return True
    return False

def get_tile_puller(tile_stat, crap_func, t_frames, z_frames):
    HRfn = tile_stat['HRfn']
    LRfn = tile_stat['LRfn']
    ftype = tile_stat['ftype']
    LRnz = tile_stat['LRnz']
    LRnt = tile_stat['LRnt']

    half_z = z_frames // 2
    half_t = t_frames // 2

    if ftype == 'czi':
        img_f = czifile.CziFile(fn)
        proc_axes, proc_shape = get_czi_shape_info(img_f)
        img_data = img_f.asarray()
        img_data = img_data.astype(np.float32)

        def czi_get(istat):
            c,z,t,x,y,mi,ma,is_uint8,rmax,all_rmax,all_ma = [istat[fld] for fld in ['c','z','t','x','y','mi','ma','uint8','rmax','all_rmax','all_ma']]
            if is_uint8:
                mi, ma, rmax = 0., 255.0, 255.0
                all_ma, all_rmax = 255.0, 255.0

            t_slice = slice(t-half_t, t+half_t+1) if half_t > 0 else t
            z_slice = slice(z-half_z, z+half_z+1) if half_z > 0 else z
            idx = build_index(
                proc_axes, {
                    'C': c,
                    'T': t_slice,
                    'Z': z_slice,
                    'X': slice(0, x),
                    'Y': slice(0, y)
                })
            img = img_data[idx].copy()
            img /= all_rmax
            if len(img.shape) <= 2: img = img[None]
            return img

        img_get = czi_get
        img_get._to_close = img_f
    else: ##e.g. if images are tif-files
        HR_pil_img = PIL.Image.open(HRfn)
        LR_pil_img = PIL.Image.open(LRfn)

        def pil_get(istat):
            c,z,t,x,y,mi,ma,is_uint8,rmax,all_rmax,all_ma = [istat[fld] for fld in ['c','LRz','LRt','LRx','LRy','LRmi','LRma','uint8','LRrmax','LRall_rmax','LRall_ma']]
            if half_t > 0: n_start, n_end = t-half_t, t+half_t+1
            elif half_z > 0: n_start, n_end = z-half_z, z+half_z+1
            else: n_start, n_end = 0,1

            if is_uint8:
                mi, ma, rmax = 0., 255.0, 255.0
                all_ma, all_rmax = 255.0, 255.0

            HR_img_array = []

            for ix in range(n_start, n_end):
                HR_pil_img.seek(ix)
                HR_pil_img.load()
                HRimg = np.array(HR_pil_img)
                if len(HRimg.shape) > 2: HRimg = HRimg[:,:,0]
                HR_img_array.append(HRimg.copy())
                
            LR_img_array = []
            for ix in range(n_start, n_end):
                LR_pil_img.seek(ix)
                LR_pil_img.load()
                LRimg = np.array(LR_pil_img)
                if len(LRimg.shape) > 2: LRimg = LRimg[:,:,0]
                LR_img_array.append(LRimg.copy())

            HRimg = np.stack(HR_img_array)
            HRimg = HRimg.astype(np.float32)
            HRimg /= all_rmax
            
            LRimg = np.stack(LR_img_array)
            LRimg = LRimg.astype(np.float32)
            LRimg /= all_rmax
            
            img = [HRimg, LRimg]
            return img  

        img_get = pil_get #img_get is full frame image
        #HR_img_get = pil_get[0]        #img_get is full frame image
        #LR_img_get = pil_get[1]
        img_get._to_close = HR_pil_img

    #puller is executed right after the above
    def puller(istat, tile_folder, crap_folder, close_me=False):
        if close_me:
            img_get._to_close.close()
            return None
        
        
        id = istat['index']
        HRfn = Path(istat['HRfn'])
        LRfn = Path(istat['LRfn'])
        tile_sz = istat['tile_sz']
        c,z,t,x,y,mi,ma,is_uint8,rmax = [istat[fld] for fld in ['c','LRz','LRt','LRx','LRy','LRmi','LRma','uint8','LRrmax']]

        raw_data = img_get(istat)
        HR_raw_data = raw_data[0]
        LR_raw_data = raw_data[1]
        
        HR_img_data = (np.iinfo(np.uint8).max * HR_raw_data).astype(np.uint8)
        LR_img_data = (np.iinfo(np.uint8).max * LR_raw_data).astype(np.uint8)
        
        #np.iinfo(np.uint8).max = maximaler Wert des Datentyps np.uint8 (0 nis 255) .... mit raw_data multiplizieren und as uint8 abspeichern
        HRthresh = np.percentile(HR_img_data, 2)
        HRthresh_pct = (HR_img_data > HRthresh).mean() * 0.30
        LRthresh = np.percentile(LR_img_data, 2)
        LRthresh_pct = (LR_img_data > LRthresh).mean() * 0.30

        HRframe_count = HR_img_data.shape[0]
        LRframe_count = LR_img_data.shape[0]
        
        HRmid_frame = HRframe_count // 2
        LRmid_frame = LRframe_count // 2
        
        if (HR_img_data.shape[0] % LR_img_data.shape[0] == 0):
            magnification = int(HR_img_data.shape[1] / LR_img_data.shape[1])

        else:
            print("Mismatch in HR-LR Image Sizes")
        SSIM = 0
        PSNR = 0
        while SSIM < 0.01:
            try:
                LR_crop_img, LRbox = draw_random_tile(LR_img_data[LRmid_frame], istat['tile_sz'], LRthresh, LRthresh_pct) ## Original HR Image Crop is done here
                ##box contains information about the tiles position in the full frame image....box = [xs.start, ys.start, xs.stop, ys.stop]            
                specTile = draw_specific_tile(HR_img_data[HRmid_frame], LR_crop_img, LRbox, magnification)
                SSIM = specTile[1]
                PSNR = specTile[2]
            except:
                print("except")
                SSIM = 0.05
                PSNR = 0.05
            
        LR_crop_img.save(crap_folder/f'{id:06d}_{LRfn.stem}.tif')
        HR_crop_img = specTile[0]
        HR_crop_img.save(tile_folder/f'{id:06d}_{HRfn.stem}.tif')
        
        
        #if crap_func and crap_folder: # Crappified IMG crop is done here
            #if frame_count > 1:
                #crap_data = []
                #for i in range(frame_count):
                    #frame_img = img_data[i, box[0]:box[2], box[1]:box[3]]
                    #crap_frame = crap_func(frame_img)
                    #crap_data.append(np.array(crap_frame))
                #multi_array = np.stack(crap_data)
                #np.save(crap_folder/f'{id:06d}_{fn.stem}.npy', multi_array)
            #else:
                #crap_img = crap_func(crop_img)
                #crap_img.save(crap_folder/f'{id:06d}_{fn.stem}.tif')

        info = dict(istat)
        info['id'] = id
        info['box'] = LRbox
        info['tile_sz'] = tile_sz
        crop_data = np.array(HR_crop_img)
        info['after_mean'] = crop_data.mean()
        info['after_sd'] = crop_data.std()
        info['after_max'] = crop_data.max()
        info['after_min'] = crop_data.min()
        info['SSIM'] = specTile[1]
        info['PSNR'] = specTile[2]
        info['LRfilename'] = f'{id:06d}_{LRfn.stem}.tif'
        info['HRfilename'] = f'{id:06d}_{HRfn.stem}.tif'
        return info

    return puller #puller itself return info

def check_info(info, t_frames, z_frames):
    t_space = t_frames // 2
    z_space = z_frames // 2

    z_ok = (info['LRnz'] >= z_frames) and (info['LRz'] >= z_space) and (info['LRz'] < (info['LRnz']-z_space))
    t_ok = (info['LRnt'] >= t_frames) and (info['LRt'] >= t_space) and (info['LRt'] < (info['LRnt']-t_space))

    return t_ok and z_ok



@call_parse
def main(out: Param("dataset folder", Path, required=True),
         info: Param('info file', Path, required=True),
         tile: Param('generated tile size', int, nargs='+', required=True),
         n_train: Param('number of train tiles', int, required=True),
         n_valid: Param('number of validation tiles', int, required=True),
         #crap_func: Param('crappifier name', str) = 'no_crap',
         n_frames: Param('number of frames', int) = 1,
         lr_type: Param('training input, (s)ingle, (t) multi or (z) multi', str) = 's',
         scale: Param('amount to scale', int) = 4,
         ftypes: Param('ftypes allowed e.g. - czi, tif', str, nargs='+') = None,
         upsample: Param('use upsample', action='store_true') = False,
         only: Param('limit to these categories', nargs='+') = None,
         skip: Param("categories to skip", str, nargs='+') = ['random', 'ArgoSIMDL'],
         clean: Param("wipe existing data first", action='store_true') = False):
    "generate tiles from source tiffs"
    up = 'up' if upsample else ''

    if lr_type not in ['s','t','z']:
        print('lr_type should be s, t or z')
        return 1

    if lr_type == 's':
        z_frames, t_frames = 1, 1
    elif lr_type == 't':
        z_frames, t_frames = 1, n_frames
    elif lr_type == 'z':
        z_frames, t_frames = n_frames, 1

    out = ensure_folder(out/f'{lr_type}_{n_frames}_{info.stem}_RW')
    if clean:
        shutil.rmtree(out)

 #   crap_func = eval(crap_func)
 #   if not crap_func is None:
 #       if not callable(crap_func):
 #           print('crap_func is not callable')
 #           crap_func = None
 #       else:
 #           crap_func = partial(crap_func, scale=scale, upsample=upsample)

    info = pd.read_csv(info)  # Info File is read

    if ftypes: info = info.loc[info.ftype.isin(ftypes)]
    if only: info = info.loc[info.category.isin(only)]
    elif skip: info = info.loc[~info.category.isin(skip)]

    info = info.loc[info.LRnz >= z_frames] #info = only info where number of z frames is bigger or equal to z_frames
    info = info.loc[info.LRnt >= t_frames] #These two have no effect in our case

    tile_infos = []
    for mode, n_samples in [('train', n_train),('valid', n_valid)]:#iterate through these two cases
        mode_info = info.loc[info.dsplit == mode] #dsplit is the column with train&valid --> mode_info is lines for training or valid
        categories = list(mode_info.groupby('category')) #create list of categories --> only one dataset category in our case
        
        files_by_category  = {c:list(info.groupby('HRfn')) for c,info in categories}  #in out case the same as mode_info

        for i in range(n_samples): #iterate through n_samples --> that is the number of training of validation images as defined in shell paremeters
            category, cat_df = random.choice(categories) #one random category --> we have only one --- cat_df is whole info
            HRfn, HRitem_df = random.choice(files_by_category[category]) #Looks at the lines in the activated catergory and pics a random one
            LRfn = HRitem_df.iloc[0].LRfn #LR image file name
            legal_choices = [item_info for ix, item_info in HRitem_df.iterrows() if check_info(item_info, t_frames, z_frames)]

            assert(legal_choices) #checks if legal_choices is True
            item_info = random.choice(legal_choices)
            for tile_sz in tile:
                item_d = dict(item_info)
                item_d['tile_sz'] = tile_sz 
                tile_infos.append(item_d) #adds tile_sz to info

    tile_info_df = pd.DataFrame(tile_infos).reset_index()
    print('num tile pulls:', len(tile_infos))
    print(tile_info_df.groupby('category').HRfn.count())

    last_stat = None
    tile_pull_info = []
    tile_puller = None
    multi_str = f'_{lr_type}_{n_frames}' if lr_type != 's' else ''
    mbar = master_bar(tile_info_df.groupby('HRfn'))
    for HRfn, tile_stats in mbar:
        #if Path(fn).stem == 'high res microtubules for testing before stitching - great quality':
        #    continue
        for i, tile_stat in progress_bar(list(tile_stats.iterrows()), parent=mbar):
            try:
                mode = tile_stat['dsplit']
                category = tile_stat['category']
                tile_sz = tile_stat['tile_sz']
                tile_folder = ensure_folder(out / f'hr_{lr_type}_{tile_sz}{multi_str}' / mode / category)
                crap_func = "lr"
                if crap_func:
                    crap_folder = ensure_folder(out / f'lr{up}_{lr_type}_{tile_sz}{multi_str}' / mode / category)
                else: crap_folder = None
                #crap_folder = None

                if need_cache_flush(tile_stat, last_stat):
                    if tile_puller:
                        tile_puller(None, None, None, close_me=True)
                    last_stat = tile_stat.copy()
                    tile_sz = tile_stat['tile_sz']
                    tile_puller = get_tile_puller(tile_stat,crap_folder,t_frames, z_frames) #tile_stat contains full pd.dataframe
                tile_pull_info.append(tile_puller(tile_stat, tile_folder, crap_folder))
                
            except MemoryError as error:
                # some files are too big to read
                fn = Path(tile_stat['HRfn'])
                print(f'too big: {fn.stem}')

    pd.DataFrame(tile_pull_info).to_csv(out/f'tiles{multi_str}.csv', index = False)
